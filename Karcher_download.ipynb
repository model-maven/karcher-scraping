{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c81a44b-376f-4d79-8881-13b53a2a643c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to kaercher_product.csv.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to get product data from the page\n",
    "def get_product_data(soup):\n",
    "    # Initialize empty variables for each field\n",
    "    product_data = {\n",
    "        \"title\": None,\n",
    "        \"description\": None,\n",
    "        \"price\": None,\n",
    "        \"image_url\": None,\n",
    "        \"features\": [],\n",
    "        \"specifications\": []\n",
    "    }\n",
    "\n",
    "    # Extract product title\n",
    "    title_tag = soup.find('h1')\n",
    "    if title_tag:\n",
    "        product_data['title'] = title_tag.text.strip()\n",
    "\n",
    "    # Extract description\n",
    "    description_tag = soup.find('p', property='description')\n",
    "    if description_tag:\n",
    "        product_data['description'] = description_tag.text.strip()\n",
    "\n",
    "    # Extract image URL\n",
    "    image_tag = soup.find('meta', property='og:image')\n",
    "    if image_tag:\n",
    "        product_data['image_url'] = image_tag['content']\n",
    "\n",
    "    # Extract features\n",
    "    features_section = soup.find('div', id='featurebenefits')\n",
    "    if features_section:\n",
    "        feature_items = features_section.find_all('div', class_='row')\n",
    "        for item in feature_items:\n",
    "            feature = item.get_text(separator=' ', strip=True)\n",
    "            if feature:\n",
    "                product_data['features'].append(feature)\n",
    "\n",
    "    # Extract specifications\n",
    "    specs_section = soup.find('div', id='specifications')\n",
    "    if specs_section:\n",
    "        spec_rows = specs_section.find_all('tr')\n",
    "        for row in spec_rows:\n",
    "            spec_title = row.find('td')\n",
    "            spec_value = spec_title.find_next_sibling('td')\n",
    "            if spec_title and spec_value:\n",
    "                product_data['specifications'].append(f\"{spec_title.text.strip()}: {spec_value.text.strip()}\")\n",
    "\n",
    "    return product_data\n",
    "\n",
    "# Function to save data into a CSV file\n",
    "def save_to_csv(data, filename):\n",
    "    df = pd.DataFrame([data])\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Main function to run the crawler\n",
    "def main():\n",
    "    url = 'https://www.kaercher.com/us/home-garden/electric-pressure-washers.html'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',\n",
    "    }\n",
    "\n",
    "    # Send a request to fetch the HTML content of the page\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    # Get the product data\n",
    "    product_data = get_product_data(soup)\n",
    "\n",
    "    # Save the data to a CSV file\n",
    "    save_to_csv(product_data, 'kaercher_product.csv')\n",
    "\n",
    "    print(f\"Data saved to kaercher_product.csv.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "787fd1c6-5add-4a39-98ae-e18b282ddd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = 'https://www.kaercher.com/us/home-garden/electric-pressure-washers.html'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all product categories by targeting the relevant class\n",
    "categories = soup.find_all('div', class_='fc-image')\n",
    "\n",
    "# Loop through each category and extract data\n",
    "for category in categories:\n",
    "    # Find the image link\n",
    "    img_tag = category.find('img')\n",
    "    img_url = img_tag['data-src'] if img_tag else 'No Image'\n",
    "    \n",
    "    # Find the product link\n",
    "    a_tag = category.find('a')\n",
    "    product_url = a_tag['href'] if a_tag else 'No URL'\n",
    "    \n",
    "    # Find the product title\n",
    "    title = a_tag.text if a_tag else 'No Title'\n",
    "    \n",
    "    # Output the scraped data\n",
    "    print(f\"Product Title: {title}\")\n",
    "    print(f\"Product URL: {product_url}\")\n",
    "    print(f\"Image URL: {img_url}\")\n",
    "    print(\"-\" * 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7877a0df-287d-4a6d-8e74-8cd1b61479d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to products.csv. 20 products found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract product data from the JSON embedded in the script tag\n",
    "def extract_product_data(json_data):\n",
    "    products = []\n",
    "    clusters = json_data.get('clusters', [])\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        for product in cluster['products']:\n",
    "            name = product.get('name', '')\n",
    "            partnumber = product.get('partnumber', '')\n",
    "            description = product.get('description', '')\n",
    "            image_url = product.get('image', '')\n",
    "            product_url = \"https://www.kaercher.com\" + product.get('url', '')\n",
    "            # Append the data to the list\n",
    "            products.append([name, partnumber, description, image_url, product_url])\n",
    "    \n",
    "    return products\n",
    "\n",
    "# Function to scrape the page and extract product information\n",
    "def scrape_products(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # Find the <script> tag containing the JSON data\n",
    "    script_tag = soup.find('script', {'type': 'application/json', 'data-kjs-setting': '_gist'})\n",
    "    \n",
    "    if script_tag:\n",
    "        json_data = json.loads(script_tag.string)\n",
    "        # Extract product data from JSON\n",
    "        products = extract_product_data(json_data)\n",
    "        return products\n",
    "    else:\n",
    "        print(\"No product data found in the script tag.\")\n",
    "        return []\n",
    "\n",
    "# Function to save product data into a CSV file\n",
    "def save_to_csv(products, filename='products.csv'):\n",
    "    df = pd.DataFrame(products, columns=['Name', 'Part Number', 'Description', 'Image URL', 'Product URL'])\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Main function to run the scraper\n",
    "def main():\n",
    "    url = 'https://www.kaercher.com/us/home-garden/electric-pressure-washers.html'  # The target URL\n",
    "    products = scrape_products(url)\n",
    "    \n",
    "    if products:\n",
    "        save_to_csv(products)\n",
    "        print(f\"Data saved to products.csv. {len(products)} products found.\")\n",
    "    else:\n",
    "        print(\"No products found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faa3a79e-2902-4abc-ad84-4e397f403c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to products_with_images.csv. 2 products found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape product details and all images from a single product page\n",
    "def scrape_product_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    # Extract product name and description\n",
    "    try:\n",
    "        name = soup.find('h1').text.strip()\n",
    "    except AttributeError:\n",
    "        name = \"Name not found\"\n",
    "\n",
    "    try:\n",
    "        description = soup.find('meta', {'name': 'description'})['content']\n",
    "    except AttributeError:\n",
    "        description = \"Description not found\"\n",
    "\n",
    "    try:\n",
    "        price = soup.find('span', class_='price').text.strip()\n",
    "    except AttributeError:\n",
    "        price = \"Price not found\"\n",
    "    \n",
    "    # Extract image URLs\n",
    "    main_image_url = soup.find('meta', {'property': 'og:image'})['content'] if soup.find('meta', {'property': 'og:image'}) else \"Main image not found\"\n",
    "    \n",
    "    # Find all images in feature benefits section\n",
    "    feature_images = [img['data-src'] for img in soup.select('div#featurebenefits img')]\n",
    "\n",
    "    # Get additional images (e.g., in specifications)\n",
    "    additional_images = [img['data-src'] for img in soup.select('div#specifications img')]\n",
    "\n",
    "    # Combine all images\n",
    "    all_images = [main_image_url] + feature_images + additional_images\n",
    "\n",
    "    # Return a structured dictionary\n",
    "    return {\n",
    "        'Product URL': url,\n",
    "        'Name': name,\n",
    "        'Description': description,\n",
    "        'Price': price,\n",
    "        'Main Image URL': main_image_url,\n",
    "        'Feature Images': ', '.join(feature_images),  # Join list into a single string\n",
    "        'Additional Images': ', '.join(additional_images)\n",
    "    }\n",
    "\n",
    "# Function to scrape multiple product URLs and store the data\n",
    "def scrape_products(urls):\n",
    "    products_data = []\n",
    "    for url in urls:\n",
    "        product_data = scrape_product_page(url)\n",
    "        products_data.append(product_data)\n",
    "    return products_data\n",
    "\n",
    "# Function to save product data to a CSV file\n",
    "def save_to_csv(products, filename='products_with_images.csv'):\n",
    "    df = pd.DataFrame(products)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Main function to run the scraper\n",
    "def main():\n",
    "    # List of product URLs\n",
    "    product_urls = [\n",
    "        \"https://www.kaercher.com/us/home-garden/electric-pressure-washers/k-5-premium-smart-control-13246830.html\",\n",
    "        \"https://www.kaercher.com/us/home-garden/electric-pressure-washers/k-5-premium-smart-control-car-home-13246840.html\",\n",
    "        # Add more URLs as needed\n",
    "    ]\n",
    "\n",
    "    # Scrape product data from all URLs\n",
    "    products = scrape_products(product_urls)\n",
    "    \n",
    "    # Save the data to CSV\n",
    "    save_to_csv(products)\n",
    "    print(f\"Data saved to products_with_images.csv. {len(products)} products found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40d75f87-6c1c-4c8e-9c51-96ef9fb6c63e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3754435580.py, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\amir.emami\\AppData\\Local\\Temp\\ipykernel_27868\\3754435580.py\"\u001b[1;36m, line \u001b[1;32m27\u001b[0m\n\u001b[1;33m    price = json_data['offers']['price']\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#Downlad Products data with \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape product details and all tabbed sections from a single product page\n",
    "def scrape_product_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    # Extract product name and description\n",
    "    try:\n",
    "        name = soup.find('h1').text.strip()\n",
    "    except AttributeError:\n",
    "        name = \"Name not found\"\n",
    "\n",
    "    try:\n",
    "        description = soup.find('meta', {'name': 'description'})['content']\n",
    "    except AttributeError:\n",
    "        description = \"Description not found\"\n",
    "\n",
    "    try:\n",
    "        scripts = soup.find_all('script', type='application/ld+json')\n",
    "        for script in scripts:\n",
    "            json_data = json.loads(script.string)\n",
    "            if 'offers' in json_data:\n",
    "            price = json_data['offers']['price']\n",
    "        break\n",
    "    else:\n",
    "        price = \"Price not found\"\n",
    "except (AttributeError, KeyError, TypeError, json.JSONDecodeError):\n",
    "    price = \"Price not found\"\n",
    "    \n",
    "    # Extract image URLs\n",
    "    main_image_url = soup.find('meta', {'property': 'og:image'})['content'] if soup.find('meta', {'property': 'og:image'}) else \"Main image not found\"\n",
    "    \n",
    "    # Find all images in feature benefits section\n",
    "    feature_images = [img['data-src'] for img in soup.select('div#featurebenefits img')]\n",
    "\n",
    "    # Extract tabbed section content\n",
    "    try:\n",
    "        description_section = soup.select_one('div#description').text.strip()\n",
    "    except AttributeError:\n",
    "        description_section = \"Description section not found\"\n",
    "    \n",
    "    try:\n",
    "        features_section = soup.select_one('div#featurebenefits').text.strip()\n",
    "    except AttributeError:\n",
    "        features_section = \"Features section not found\"\n",
    "    \n",
    "    try:\n",
    "        specifications_section = soup.select_one('div#specifications').text.strip()\n",
    "    except AttributeError:\n",
    "        specifications_section = \"Specifications section not found\"\n",
    "    \n",
    "    try:\n",
    "        documents_section = soup.select_one('div#downloads').text.strip()\n",
    "    except AttributeError:\n",
    "        documents_section = \"Documents section not found\"\n",
    "    \n",
    "    try:\n",
    "        videos_section = soup.select_one('div#videos').text.strip()\n",
    "    except AttributeError:\n",
    "        videos_section = \"Videos section not found\"\n",
    "    \n",
    "    try:\n",
    "        application_section = soup.select_one('div#application').text.strip()\n",
    "    except AttributeError:\n",
    "        application_section = \"Application section not found\"\n",
    "    \n",
    "    try:\n",
    "        accessories_section = soup.select_one('div#accessory').text.strip()\n",
    "    except AttributeError:\n",
    "        accessories_section = \"Accessories section not found\"\n",
    "    \n",
    "    try:\n",
    "        detergents_section = soup.select_one('div#detergent').text.strip()\n",
    "    except AttributeError:\n",
    "        detergents_section = \"Detergents section not found\"\n",
    "    \n",
    "    try:\n",
    "        parts_section = soup.select_one('div#spareparts').text.strip()\n",
    "    except AttributeError:\n",
    "        parts_section = \"Parts section not found\"\n",
    "    \n",
    "    try:\n",
    "        ratings_section = soup.select_one('div#ratings').text.strip()\n",
    "    except AttributeError:\n",
    "        ratings_section = \"Ratings section not found\"\n",
    "\n",
    "    # Return a structured dictionary\n",
    "    return {\n",
    "        'Product URL': url,\n",
    "        'Name': name,\n",
    "        'Description': description,\n",
    "        'Price': price,\n",
    "        'Main Image URL': main_image_url,\n",
    "        'Feature Images': ', '.join(feature_images),  # Join list into a single string\n",
    "        'Description Section': description_section,\n",
    "        'Features Section': features_section,\n",
    "        'Specifications Section': specifications_section,\n",
    "        'Documents Section': documents_section,\n",
    "        'Videos Section': videos_section,\n",
    "        'Application Section': application_section,\n",
    "        'Accessories Section': accessories_section,\n",
    "        'Detergents Section': detergents_section,\n",
    "        'Parts Section': parts_section,\n",
    "        'Ratings Section': ratings_section\n",
    "    }\n",
    "\n",
    "# Function to scrape multiple product URLs and store the data\n",
    "def scrape_products(urls):\n",
    "    products_data = []\n",
    "    for url in urls:\n",
    "        product_data = scrape_product_page(url)\n",
    "        products_data.append(product_data)\n",
    "    return products_data\n",
    "\n",
    "# Function to save product data to a CSV file\n",
    "def save_to_csv(products, filename='products_with_tabs.csv'):\n",
    "    df = pd.DataFrame(products)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Main function to run the scraper\n",
    "def main():\n",
    "    # List of product URLs\n",
    "    product_urls = [\n",
    "        \"https://www.kaercher.com/us/home-garden/electric-pressure-washers/k-4-power-control-13240450.html\",\n",
    "        \"https://www.kaercher.com/us/home-garden/electric-pressure-washers/k-3-power-control-16761090.html\",\n",
    "        # Add more URLs as needed\n",
    "    ]\n",
    "\n",
    "    # Scrape product data from all URLs\n",
    "    products = scrape_products(product_urls)\n",
    "    \n",
    "    # Save the data to CSV\n",
    "    save_to_csv(products)\n",
    "    print(f\"Data saved to products_with_tabs.csv. {len(products)} products found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed364de4-df59-4bde-865d-923e2fa8b0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sitemap downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the sitemap\n",
    "url = 'https://www.kaercher.com/us/sitemap1.xml'\n",
    "\n",
    "# Send an HTTP GET request to download the file\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Save the file locally\n",
    "    with open('sitemap1.xml', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(\"Sitemap downloaded successfully!\")\n",
    "else:\n",
    "    print(f\"Failed to download the sitemap. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37884e6f-9017-4c0c-a7d5-e0e70bd93988",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ace_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31580\\458117322.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# Display the first few rows of the DataFrame to verify\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mace_tools\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtools\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay_dataframe_to_user\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Karcher Products\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Product data has been exported to {output_file}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ace_tools'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Fetch the sitemap XML\n",
    "sitemap_url = 'https://www.kaercher.com/us/sitemap1.xml'\n",
    "response = requests.get(sitemap_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    sitemap_xml = response.content\n",
    "\n",
    "    # Step 2: Parse the XML data\n",
    "    root = ET.fromstring(sitemap_xml)\n",
    "\n",
    "    # Initialize a list to hold product information\n",
    "    product_data = []\n",
    "\n",
    "    # Step 3: Extract product URLs or relevant data\n",
    "    # The sitemap XML is typically structured with <url> and <loc> tags\n",
    "    for url in root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}url'):\n",
    "        loc = url.find('{http://www.sitemaps.org/schemas/sitemap/0.9}loc').text\n",
    "        product_data.append({'Product URL': loc})\n",
    "\n",
    "    # Step 4: Convert to a DataFrame and export as CSV\n",
    "    df = pd.DataFrame(product_data)\n",
    "\n",
    "    # Exporting DataFrame to CSV\n",
    "    output_file = 'kaercher_products.csv'  # Modify this path if needed\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    # Display the first few rows of the DataFrame to verify\n",
    "    import ace_tools as tools; tools.display_dataframe_to_user(name=\"Karcher Products\", dataframe=df)\n",
    "\n",
    "    print(f\"Product data has been exported to {output_file}.\")\n",
    "else:\n",
    "    print(f\"Failed to fetch the sitemap. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e67af2b2-f160-4378-87f2-321a35f218fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to products_with_tabs.csv. 2 products found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def scrape_product_page(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Check for request issues\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "        # Extract product name and description\n",
    "        try:\n",
    "            name = soup.find('h1').text.strip()\n",
    "        except AttributeError:\n",
    "            name = \"Name not found\"\n",
    "\n",
    "        try:\n",
    "            description = soup.find('meta', {'name': 'description'})['content']\n",
    "        except AttributeError:\n",
    "            description = \"Description not found\"\n",
    "\n",
    "        # Extract price from JSON-LD script\n",
    "        try:\n",
    "            scripts = soup.find_all('script', type='application/ld+json')\n",
    "            for script in scripts:\n",
    "                json_data = json.loads(script.string)\n",
    "                if 'offers' in json_data:\n",
    "                    price = json_data['offers']['price']\n",
    "                    break\n",
    "            else:\n",
    "                price = \"Price not found\"\n",
    "        except (AttributeError, KeyError, TypeError, json.JSONDecodeError):\n",
    "            price = \"Price not found\"\n",
    "\n",
    "        # Extract image URLs\n",
    "        main_image_url = soup.find('meta', {'property': 'og:image'})['content'] if soup.find('meta', {'property': 'og:image'}) else \"Main image not found\"\n",
    "        \n",
    "        # Find all images in feature benefits section\n",
    "        feature_images = [img['data-src'] for img in soup.select('div#featurebenefits img')]\n",
    "\n",
    "        # Extract tabbed section content (similar logic for other sections)\n",
    "        try:\n",
    "            description_section = soup.select_one('div#description').text.strip()\n",
    "        except AttributeError:\n",
    "            description_section = \"Description section not found\"\n",
    "        \n",
    "        try:\n",
    "            features_section = soup.select_one('div#featurebenefits').text.strip()\n",
    "        except AttributeError:\n",
    "            features_section = \"Features section not found\"\n",
    "        \n",
    "        try:\n",
    "            specifications_section = soup.select_one('div#specifications').text.strip()\n",
    "        except AttributeError:\n",
    "            specifications_section = \"Specifications section not found\"\n",
    "        \n",
    "        try:\n",
    "            documents_section = soup.select_one('div#downloads').text.strip()\n",
    "        except AttributeError:\n",
    "            documents_section = \"Documents section not found\"\n",
    "        \n",
    "        try:\n",
    "            videos_section = soup.select_one('div#videos').text.strip()\n",
    "        except AttributeError:\n",
    "            videos_section = \"Videos section not found\"\n",
    "        \n",
    "        try:\n",
    "            application_section = soup.select_one('div#application').text.strip()\n",
    "        except AttributeError:\n",
    "            application_section = \"Application section not found\"\n",
    "        \n",
    "        try:\n",
    "            accessories_section = soup.select_one('div#accessory').text.strip()\n",
    "        except AttributeError:\n",
    "            accessories_section = \"Accessories section not found\"\n",
    "        \n",
    "        try:\n",
    "            detergents_section = soup.select_one('div#detergent').text.strip()\n",
    "        except AttributeError:\n",
    "            detergents_section = \"Detergents section not found\"\n",
    "        \n",
    "        try:\n",
    "            parts_section = soup.select_one('div#spareparts').text.strip()\n",
    "        except AttributeError:\n",
    "            parts_section = \"Parts section not found\"\n",
    "        \n",
    "        try:\n",
    "            ratings_section = soup.select_one('div#ratings').text.strip()\n",
    "        except AttributeError:\n",
    "            ratings_section = \"Ratings section not found\"\n",
    "\n",
    "        return {\n",
    "            'Product URL': url,\n",
    "            'Name': name,\n",
    "            'Description': description,\n",
    "            'Price': price,\n",
    "            'Main Image URL': main_image_url,\n",
    "            'Feature Images': ', '.join(feature_images),\n",
    "            'Description Section': description_section,\n",
    "            'Features Section': features_section,\n",
    "            'Specifications Section': specifications_section,\n",
    "            'Documents Section': documents_section,\n",
    "            'Videos Section': videos_section,\n",
    "            'Application Section': application_section,\n",
    "            'Accessories Section': accessories_section,\n",
    "            'Detergents Section': detergents_section,\n",
    "            'Parts Section': parts_section,\n",
    "            'Ratings Section': ratings_section\n",
    "        }\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def scrape_products(urls):\n",
    "    products_data = []\n",
    "    for url in urls:\n",
    "        product_data = scrape_product_page(url)\n",
    "        if product_data:\n",
    "            products_data.append(product_data)\n",
    "    return products_data\n",
    "\n",
    "\n",
    "def save_to_csv(products, filename='products_with_tabs.csv'):\n",
    "    df = pd.DataFrame(products)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "    product_urls = [\n",
    "        \"https://www.kaercher.com/us/home-garden/electric-pressure-washers/k-4-power-control-13240450.html\",\n",
    "        \"https://www.kaercher.com/us/home-garden/electric-pressure-washers/k-3-power-control-16761090.html\"\n",
    "    ]\n",
    "\n",
    "    products = scrape_products(product_urls)\n",
    "    save_to_csv(products)\n",
    "    print(f\"Data saved to products_with_tabs.csv. {len(products)} products found.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00c213de-9b19-4507-b44e-8fb1b8df61bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to products_with_tabs.csv. 2 products found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def scrape_product_page(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "        # Extract product name and description\n",
    "        try:\n",
    "            name = soup.find('h1').text.strip()\n",
    "        except AttributeError:\n",
    "            name = \"Name not found\"\n",
    "\n",
    "        try:\n",
    "            description = soup.find('meta', {'name': 'description'})['content']\n",
    "        except AttributeError:\n",
    "            description = \"Description not found\"\n",
    "\n",
    "        # Extract price from JSON-LD script\n",
    "        try:\n",
    "            scripts = soup.find_all('script', type='application/ld+json')\n",
    "            for script in scripts:\n",
    "                json_data = json.loads(script.string)\n",
    "                if 'offers' in json_data:\n",
    "                    price = json_data['offers']['price']\n",
    "                    break\n",
    "            else:\n",
    "                price = \"Price not found\"\n",
    "        except (AttributeError, KeyError, TypeError, json.JSONDecodeError):\n",
    "            price = \"Price not found\"\n",
    "\n",
    "        # Extract image URLs\n",
    "        main_image_url = soup.find('meta', {'property': 'og:image'})['content'] if soup.find('meta', {'property': 'og:image'}) else \"Main image not found\"\n",
    "        \n",
    "        # Find all images in feature benefits section\n",
    "        feature_images = [img['data-src'] for img in soup.select('div#featurebenefits img')]\n",
    "\n",
    "        # Extract specifications\n",
    "        specifications = {}\n",
    "        try:\n",
    "            spec_table_rows = soup.select('div#specifications table tr')\n",
    "            for row in spec_table_rows:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) == 2:\n",
    "                    spec_key = cells[0].text.strip()\n",
    "                    spec_value = cells[1].text.strip()\n",
    "                    specifications[spec_key] = spec_value\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        # Extract tabbed section content (same logic for other sections)\n",
    "        try:\n",
    "            description_section = soup.select_one('div#description').text.strip()\n",
    "        except AttributeError:\n",
    "            description_section = \"Description section not found\"\n",
    "        \n",
    "        try:\n",
    "            features_section = soup.select_one('div#featurebenefits').text.strip()\n",
    "        except AttributeError:\n",
    "            features_section = \"Features section not found\"\n",
    "        \n",
    "        try:\n",
    "            specifications_section = soup.select_one('div#specifications').text.strip()\n",
    "        except AttributeError:\n",
    "            specifications_section = \"Specifications section not found\"\n",
    "        \n",
    "        try:\n",
    "            documents_section = soup.select_one('div#downloads').text.strip()\n",
    "        except AttributeError:\n",
    "            documents_section = \"Documents section not found\"\n",
    "        \n",
    "        try:\n",
    "            videos_section = soup.select_one('div#videos').text.strip()\n",
    "        except AttributeError:\n",
    "            videos_section = \"Videos section not found\"\n",
    "        \n",
    "        try:\n",
    "            application_section = soup.select_one('div#application').text.strip()\n",
    "        except AttributeError:\n",
    "            application_section = \"Application section not found\"\n",
    "        \n",
    "        try:\n",
    "            accessories_section = soup.select_one('div#accessory').text.strip()\n",
    "        except AttributeError:\n",
    "            accessories_section = \"Accessories section not found\"\n",
    "        \n",
    "        try:\n",
    "            detergents_section = soup.select_one('div#detergent').text.strip()\n",
    "        except AttributeError:\n",
    "            detergents_section = \"Detergents section not found\"\n",
    "        \n",
    "        try:\n",
    "            parts_section = soup.select_one('div#spareparts').text.strip()\n",
    "        except AttributeError:\n",
    "            parts_section = \"Parts section not found\"\n",
    "        \n",
    "        try:\n",
    "            ratings_section = soup.select_one('div#ratings').text.strip()\n",
    "        except AttributeError:\n",
    "            ratings_section = \"Ratings section not found\"\n",
    "\n",
    "        # Create dictionary with product data\n",
    "        product_data = {\n",
    "            'Product URL': url,\n",
    "            'Name': name,\n",
    "            'Description': description,\n",
    "            'Price': price,\n",
    "            'Main Image URL': main_image_url,\n",
    "            'Feature Images': ', '.join(feature_images),\n",
    "            'Description Section': description_section,\n",
    "            'Features Section': features_section,\n",
    "            'Specifications Section': specifications_section,\n",
    "            'Documents Section': documents_section,\n",
    "            'Videos Section': videos_section,\n",
    "            'Application Section': application_section,\n",
    "            'Accessories Section': accessories_section,\n",
    "            'Detergents Section': detergents_section,\n",
    "            'Parts Section': parts_section,\n",
    "            'Ratings Section': ratings_section\n",
    "        }\n",
    "\n",
    "        # Add specifications as separate columns\n",
    "        product_data.update(specifications)\n",
    "\n",
    "        return product_data\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def scrape_products(urls):\n",
    "    products_data = []\n",
    "    for url in urls:\n",
    "        product_data = scrape_product_page(url)\n",
    "        if product_data:\n",
    "            products_data.append(product_data)\n",
    "    return products_data\n",
    "\n",
    "\n",
    "def save_to_csv(products, filename='products_with_tabs.csv'):\n",
    "    df = pd.DataFrame(products)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "    product_urls = [\n",
    "        \"https://www.kaercher.com/us/accessories/suction-bar-carpet-cleaning-50332750.html\",\n",
    "        \"https://www.kaercher.com/us/accessories/squeegee-47770800.html\"\n",
    "    ]\n",
    "\n",
    "    products = scrape_products(product_urls)\n",
    "    save_to_csv(products)\n",
    "    print(f\"Data saved to products_with_tabs.csv. {len(products)} products found.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b90cb27d-24c1-4d5a-8e15-7c88ee16b572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to products_with_tabs.csv. 2 products found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def scrape_product_page(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "        # Extract product name and description\n",
    "        try:\n",
    "            name = soup.find('h1').text.strip()\n",
    "        except AttributeError:\n",
    "            name = \"Name not found\"\n",
    "\n",
    "        try:\n",
    "            description = soup.find('meta', {'name': 'description'})['content']\n",
    "        except AttributeError:\n",
    "            description = \"Description not found\"\n",
    "\n",
    "        # Initialize price and part number with default values\n",
    "        price = \"Price not found\"\n",
    "        part_number = \"Part number not found\"\n",
    "\n",
    "        # Look for the structured JSON-LD data\n",
    "        try:\n",
    "            scripts = soup.find_all('script', type='application/ld+json')\n",
    "            for script in scripts:\n",
    "                json_data = json.loads(script.string)\n",
    "                \n",
    "                # Look for price under the 'offers' key\n",
    "                if 'offers' in json_data and 'price' in json_data['offers']:\n",
    "                    price = json_data['offers']['price']\n",
    "                \n",
    "                # Look for part number in 'sku' or 'mpn'\n",
    "                if 'sku' in json_data:\n",
    "                    part_number = json_data['sku']\n",
    "                elif 'mpn' in json_data:\n",
    "                    part_number = json_data['mpn']\n",
    "                \n",
    "                # If both price and part_number are found, break\n",
    "                if price != \"Price not found\" and part_number != \"Part number not found\":\n",
    "                    break\n",
    "\n",
    "        except (AttributeError, KeyError, TypeError, json.JSONDecodeError):\n",
    "            pass\n",
    "\n",
    "        # Extract image URLs\n",
    "        main_image_url = soup.find('meta', {'property': 'og:image'})['content'] if soup.find('meta', {'property': 'og:image'}) else \"Main image not found\"\n",
    "        \n",
    "        # Find all images in feature benefits section\n",
    "        feature_images = [img['data-src'] for img in soup.select('div#featurebenefits img')]\n",
    "\n",
    "        # Extract specifications\n",
    "        specifications = {}\n",
    "        try:\n",
    "            spec_table_rows = soup.select('div#specifications table tr')\n",
    "            for row in spec_table_rows:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) == 2:\n",
    "                    spec_key = cells[0].text.strip()\n",
    "                    spec_value = cells[1].text.strip()\n",
    "                    specifications[spec_key] = spec_value\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        # Extract tabbed section content (similar logic for other sections)\n",
    "        try:\n",
    "            description_section = soup.select_one('div#description').text.strip()\n",
    "        except AttributeError:\n",
    "            description_section = \"Description section not found\"\n",
    "        \n",
    "        try:\n",
    "            features_section = soup.select_one('div#featurebenefits').text.strip()\n",
    "        except AttributeError:\n",
    "            features_section = \"Features section not found\"\n",
    "        \n",
    "        try:\n",
    "            specifications_section = soup.select_one('div#specifications').text.strip()\n",
    "        except AttributeError:\n",
    "            specifications_section = \"Specifications section not found\"\n",
    "        \n",
    "        try:\n",
    "            documents_section = soup.select_one('div#downloads').text.strip()\n",
    "        except AttributeError:\n",
    "            documents_section = \"Documents section not found\"\n",
    "        \n",
    "        try:\n",
    "            videos_section = soup.select_one('div#videos').text.strip()\n",
    "        except AttributeError:\n",
    "            videos_section = \"Videos section not found\"\n",
    "        \n",
    "        try:\n",
    "            application_section = soup.select_one('div#application').text.strip()\n",
    "        except AttributeError:\n",
    "            application_section = \"Application section not found\"\n",
    "        \n",
    "        try:\n",
    "            accessories_section = soup.select_one('div#accessory').text.strip()\n",
    "        except AttributeError:\n",
    "            accessories_section = \"Accessories section not found\"\n",
    "        \n",
    "        try:\n",
    "            detergents_section = soup.select_one('div#detergent').text.strip()\n",
    "        except AttributeError:\n",
    "            detergents_section = \"Detergents section not found\"\n",
    "        \n",
    "        try:\n",
    "            parts_section = soup.select_one('div#spareparts').text.strip()\n",
    "        except AttributeError:\n",
    "            parts_section = \"Parts section not found\"\n",
    "        \n",
    "        try:\n",
    "            ratings_section = soup.select_one('div#ratings').text.strip()\n",
    "        except AttributeError:\n",
    "            ratings_section = \"Ratings section not found\"\n",
    "\n",
    "        # Create dictionary with product data\n",
    "        product_data = {\n",
    "            'Product URL': url,\n",
    "            'Name': name,\n",
    "            'Part Number': part_number,  # Adding Part Number\n",
    "            'Description': description,\n",
    "            'Price': price,\n",
    "            'Main Image URL': main_image_url,\n",
    "            'Feature Images': ', '.join(feature_images),\n",
    "            'Description Section': description_section,\n",
    "            'Features Section': features_section,\n",
    "            'Specifications Section': specifications_section,\n",
    "            'Documents Section': documents_section,\n",
    "            'Videos Section': videos_section,\n",
    "            'Application Section': application_section,\n",
    "            'Accessories Section': accessories_section,\n",
    "            'Detergents Section': detergents_section,\n",
    "            'Parts Section': parts_section,\n",
    "            'Ratings Section': ratings_section\n",
    "        }\n",
    "\n",
    "        # Add specifications as separate columns\n",
    "        product_data.update(specifications)\n",
    "\n",
    "        return product_data\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def scrape_products(urls):\n",
    "    products_data = []\n",
    "    for url in urls:\n",
    "        product_data = scrape_product_page(url)\n",
    "        if product_data:\n",
    "            products_data.append(product_data)\n",
    "    return products_data\n",
    "\n",
    "\n",
    "def save_to_csv(products, filename='products_with_tabs.csv'):\n",
    "    df = pd.DataFrame(products)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "    product_urls = [\n",
    "        \"https://www.kaercher.com/us/home-garden/window-vac.html\",\n",
    "        \"https://www.kaercher.com/us/home-garden/window-vac/wv-6-plus-white-16337450.html\"\n",
    "    ]\n",
    "\n",
    "    products = scrape_products(product_urls)\n",
    "    save_to_csv(products)\n",
    "    print(f\"Data saved to products_with_tabs.csv. {len(products)} products found.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6b03feec-edf0-4898-a66e-16bd512e4575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to products_with_tabs.csv. 2 products found.\n"
     ]
    }
   ],
   "source": [
    "#Product download without catalog\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def scrape_product_page(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "        # Extract product name and description\n",
    "        try:\n",
    "            name = soup.find('h1').text.strip()\n",
    "        except AttributeError:\n",
    "            name = \"Name not found\"\n",
    "\n",
    "        try:\n",
    "            description = soup.find('meta', {'name': 'description'})['content']\n",
    "        except AttributeError:\n",
    "            description = \"Description not found\"\n",
    "\n",
    "        # Initialize price and part number with default values\n",
    "        price = \"Price not found\"\n",
    "        part_number = \"Part number not found\"\n",
    "\n",
    "        # Look for the structured JSON-LD data\n",
    "        try:\n",
    "            scripts = soup.find_all('script', type='application/ld+json')\n",
    "            for script in scripts:\n",
    "                json_data = json.loads(script.string)\n",
    "                \n",
    "                # Look for price under the 'offers' key\n",
    "                if 'offers' in json_data and 'price' in json_data['offers']:\n",
    "                    price = json_data['offers']['price']\n",
    "                \n",
    "                # Look for part number in 'sku' or 'mpn'\n",
    "                if 'sku' in json_data:\n",
    "                    part_number = json_data['sku']\n",
    "                elif 'mpn' in json_data:\n",
    "                    part_number = json_data['mpn']\n",
    "                \n",
    "                # If both price and part_number are found, break\n",
    "                if price != \"Price not found\" and part_number != \"Part number not found\":\n",
    "                    break\n",
    "\n",
    "        except (AttributeError, KeyError, TypeError, json.JSONDecodeError):\n",
    "            pass\n",
    "\n",
    "        # Extract image URLs\n",
    "        main_image_url = soup.find('meta', {'property': 'og:image'})['content'] if soup.find('meta', {'property': 'og:image'}) else \"Main image not found\"\n",
    "        \n",
    "        # Find all images in feature benefits section\n",
    "        feature_images = [img['data-src'] for img in soup.select('div#featurebenefits img')]\n",
    "\n",
    "        # Extract specifications\n",
    "        specifications = {}\n",
    "        try:\n",
    "            spec_table_rows = soup.select('div#specifications table tr')\n",
    "            for row in spec_table_rows:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) == 2:\n",
    "                    spec_key = cells[0].text.strip()\n",
    "                    spec_value = cells[1].text.strip()\n",
    "                    specifications[spec_key] = spec_value\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        # Create dictionary with product data\n",
    "        product_data = {\n",
    "            'Product URL': url,\n",
    "            'Name': name,\n",
    "            'Part Number': part_number,  # Adding Part Number\n",
    "            'Description': description,\n",
    "            'Price': price,\n",
    "            'Main Image URL': main_image_url,\n",
    "            'Feature Images': ', '.join(feature_images),\n",
    "                  }\n",
    "\n",
    "        # Add specifications as separate columns\n",
    "        product_data.update(specifications)\n",
    "\n",
    "        return product_data\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def scrape_products(urls):\n",
    "    products_data = []\n",
    "    for url in urls:\n",
    "        product_data = scrape_product_page(url)\n",
    "        if product_data:\n",
    "            products_data.append(product_data)\n",
    "    return products_data\n",
    "\n",
    "\n",
    "def save_to_csv(products, filename='products_with_tabs.csv'):\n",
    "    df = pd.DataFrame(products)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "    product_urls = [\n",
    "        \"https://www.kaercher.com/us/home-garden/electric-pressure-washers/k-4-power-control-13240450.html\",\n",
    "        \"https://www.kaercher.com/us/home-garden/electric-pressure-washers/k-3-power-control-16761090.html\"\n",
    "    ]\n",
    "\n",
    "    products = scrape_products(product_urls)\n",
    "    save_to_csv(products)\n",
    "    print(f\"Data saved to products_with_tabs.csv. {len(products)} products found.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f9b3b3e9-337d-4e51-a7ed-6dc4aec6b373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to products_with_tabs.csv. 2 products found.\n"
     ]
    }
   ],
   "source": [
    "#Product Download with pdf catalogs\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def scrape_product_page(url):\n",
    "    try:\n",
    "        # Setup Selenium with Chrome driver\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--headless')  # Run Chrome in headless mode\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        \n",
    "        service = Service('C:\\\\Users\\\\amir.emami\\\\AppData\\\\Local\\\\Temp\\\\Rar$EXa36532.5574.rartemp\\\\chromedriver-win32\\\\chromedriver.exe')\n",
    "\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        \n",
    "        driver.get(url)\n",
    "        \n",
    "        # Introduce delay to simulate human-like page load and interaction\n",
    "        time.sleep(5)  # Wait for 5 seconds, simulating human wait for the page to load\n",
    "\n",
    "        # Get the page source and process it with BeautifulSoup\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "        # Close the browser session\n",
    "        driver.quit()\n",
    "\n",
    "        # Extract product name and description\n",
    "        try:\n",
    "            name = soup.find('h1').text.strip()\n",
    "        except AttributeError:\n",
    "            name = \"Name not found\"\n",
    "\n",
    "        try:\n",
    "            description = soup.find('meta', {'name': 'description'})['content']\n",
    "        except AttributeError:\n",
    "            description = \"Description not found\"\n",
    "\n",
    "        # Initialize price and part number with default values\n",
    "        price = \"Price not found\"\n",
    "        part_number = \"Part number not found\"\n",
    "\n",
    "        # Look for the structured JSON-LD data\n",
    "        try:\n",
    "            scripts = soup.find_all('script', type='application/ld+json')\n",
    "            for script in scripts:\n",
    "                json_data = json.loads(script.string)\n",
    "                \n",
    "                # Look for price under the 'offers' key\n",
    "                if 'offers' in json_data and 'price' in json_data['offers']:\n",
    "                    price = json_data['offers']['price']\n",
    "                \n",
    "                # Look for part number in 'sku' or 'mpn'\n",
    "                if 'sku' in json_data:\n",
    "                    part_number = json_data['sku']\n",
    "                elif 'mpn' in json_data:\n",
    "                    part_number = json_data['mpn']\n",
    "                \n",
    "                # If both price and part_number are found, break\n",
    "                if price != \"Price not found\" and part_number != \"Part number not found\":\n",
    "                    break\n",
    "\n",
    "        except (AttributeError, KeyError, TypeError, json.JSONDecodeError):\n",
    "            pass\n",
    "\n",
    "        # Extract image URLs\n",
    "        main_image_url = soup.find('meta', {'property': 'og:image'})['content'] if soup.find('meta', {'property': 'og:image'}) else \"Main image not found\"\n",
    "        \n",
    "        # Find all images in feature benefits section\n",
    "        feature_images = [img['data-src'] for img in soup.select('div#featurebenefits img')]\n",
    "\n",
    "        # Extract documents (Product Brochure, Operating Instructions)\n",
    "        product_brochure_url = \"Product Brochure not found\"\n",
    "        operating_instruction_url_1 = \"Operating instructions not found\"\n",
    "        operating_instruction_url_2 = \"Operating instructions not found\"\n",
    "\n",
    "        try:\n",
    "            documents_section = soup.select('div.fc-document')\n",
    "            for document in documents_section:\n",
    "                doc_title = document.select_one('div.fc-title h6').text.strip()\n",
    "                doc_link = document.select_one('a.trk-download')['href']\n",
    "\n",
    "                if \"Product Brochure\" in doc_title:\n",
    "                    product_brochure_url = doc_link\n",
    "                elif \"Operating instructions\" in doc_title and \".pdf\" in doc_link:\n",
    "                    operating_instruction_url_1 = doc_link\n",
    "                elif \"Operating instructions\" in doc_title and \".html\" in doc_link:\n",
    "                    operating_instruction_url_2 = doc_link\n",
    "\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        # Extract specifications\n",
    "        specifications = {}\n",
    "        try:\n",
    "            spec_table_rows = soup.select('div#specifications table tr')\n",
    "            for row in spec_table_rows:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) == 2:\n",
    "                    spec_key = cells[0].text.strip()\n",
    "                    spec_value = cells[1].text.strip()\n",
    "                    specifications[spec_key] = spec_value\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        # Create dictionary with product data\n",
    "        product_data = {\n",
    "            'Product URL': url,\n",
    "            'Name': name,\n",
    "            'Part Number': part_number,  # Adding Part Number\n",
    "            'Description': description,\n",
    "            'Price': price,\n",
    "            'Main Image URL': main_image_url,\n",
    "            'Feature Images': ', '.join(feature_images),\n",
    "            'Product Brochure URL': product_brochure_url,\n",
    "            'Operating Instructions PDF URL': operating_instruction_url_1,\n",
    "            'Operating Instructions HTML URL': operating_instruction_url_2\n",
    "        }\n",
    "\n",
    "        # Add specifications as separate columns\n",
    "        product_data.update(specifications)\n",
    "\n",
    "        return product_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def scrape_products(urls):\n",
    "    products_data = []\n",
    "    for url in urls:\n",
    "        product_data = scrape_product_page(url)\n",
    "        if product_data:\n",
    "            products_data.append(product_data)\n",
    "    return products_data\n",
    "\n",
    "def save_to_csv(products, filename='products_with_tabs.csv'):\n",
    "    df = pd.DataFrame(products)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "def main():\n",
    "    product_urls = [\n",
    "        \"https://www.kaercher.com/us/home-garden/electric-pressure-washers/k-4-power-control-13240450.html\",\n",
    "        \"https://www.kaercher.com/us/home-garden/electric-pressure-washers/k-3-power-control-16761090.html\"\n",
    "    ]\n",
    "\n",
    "    products = scrape_products(product_urls)\n",
    "    save_to_csv(products)\n",
    "    print(f\"Data saved to products_with_tabs.csv. {len(products)} products found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e320221-35ee-4279-8c51-485b7daee237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a0e749-696c-45f0-bea6-49daffbdda19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
